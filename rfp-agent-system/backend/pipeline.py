# Complete RFP Processing Pipeline
# Uses: Azure Blob Storage, Azure Document Intelligence, Azure OpenAI (GPT-4o)

import os
from pathlib import Path
import sys
from dotenv import load_dotenv
from document_processing.extract_text import extract_text_from_blob, extract_text_from_pdf
from document_processing.chunking import chunk_text
from embedding.embedder import generate_embedding
from azure_openai_orchestrator import AzureOpenAIOrchestrator

# Load environment variables
load_dotenv()


def save_to_kb(memory_output: dict, output_file: str = "kb.md"):
    """
    Save agent analysis results to structured knowledge base file.
    
    Args:
        memory_output: Dict of agent results
        output_file: Output filename (default: kb.md)
    """
    kb_path = Path(__file__).parent / "data/context" / output_file
    with open(kb_path, "w", encoding="utf-8") as f:
        # Header
        f.write("# Knowledge Base for RFP Analysis\n\n")
        f.write("*Generated by RFP Process Enhancer - AI Agent System*\n\n")
        f.write("---\n\n")
        
        # 1. Introduction (from introduction agent)
        if "introduction" in memory_output:
            f.write(memory_output["introduction"]["result"] + "\n\n")
            f.write("---\n\n")
        
        # 2. Requirements Section
        f.write("## 2. Requirements\n\n")
        
        # 2.1 Challenges
        if "challenges" in memory_output:
            f.write(memory_output["challenges"]["result"] + "\n\n")
        
        # 2.2 User Pain Points
        if "pain_points" in memory_output:
            f.write(memory_output["pain_points"]["result"] + "\n\n")
        
        # 2.3 Current Business Process
        if "business_process" in memory_output:
            f.write(memory_output["business_process"]["result"] + "\n\n")
        
        # 2.4 Gap Analysis
        if "gap" in memory_output:
            f.write(memory_output["gap"]["result"] + "\n\n")
        
        # 2.5 Personas
        if "personas" in memory_output:
            f.write(memory_output["personas"]["result"] + "\n\n")
        
        # 2.6 Constraints
        if "constraints" in memory_output:
            f.write(memory_output["constraints"]["result"] + "\n\n")
        
        # 2.7 Functional Requirements
        if "functional_requirements" in memory_output:
            f.write(memory_output["functional_requirements"]["result"] + "\n\n")
        
        # 2.8 Non-Functional Requirements
        if "nfr" in memory_output:
            f.write(memory_output["nfr"]["result"] + "\n\n")
        
        f.write("---\n\n")
        
        # 3. Solutioning Section
        f.write("## 3. Solutioning\n\n")
        
        # 3.1 Architecture (comprehensive from architect agent)
        if "architecture" in memory_output:
            f.write(memory_output["architecture"]["result"] + "\n\n")
        
        f.write("---\n\n")
        
        # 4. Assumptions and Dependencies
        if "assumptions" in memory_output:
            f.write(memory_output["assumptions"]["result"] + "\n\n")
        
        f.write("---\n\n")
        
        # Appendix: Impact Statements (supporting information)
        if "impact" in memory_output:
            f.write("## Appendix: Impactful Business Statements\n\n")
            f.write("*This section contains key metrics, compliance requirements, and strategic statements extracted from the RFP.*\n\n")
            f.write(memory_output["impact"]["result"] + "\n\n")

async def process_rfp_document(blob_name: str = None, file_path: str = None):
    """
    Complete pipeline to process RFP document
    
    Args:
        blob_name: Name of blob in Azure Storage (if using Blob Storage)
        file_path: Local file path (if not using Blob Storage)
        
    Returns:
        dict: Analysis results from all agents
    """
    print("=" * 60)
    print("RFP PROCESSING PIPELINE")
    print("=" * 60)
    
    # Step 1: Extract text from document
    print("\n[1/5] Extracting text from document...")
    if blob_name:
        try:
            text = extract_text_from_blob(blob_name)
            print(f"✓ Extracted text from blob: {blob_name}")
        except Exception as e:
            print(f"✗ Error extracting from blob: {e}")
            print("Falling back to local file if provided...")
            if file_path:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                raise
    elif file_path:
        # For PDF files, read bytes and process directly (no blob upload needed)
        if file_path.lower().endswith('.pdf'):
            try:
                print(f"Reading PDF file...")
                with open(file_path, 'rb') as f:
                    file_bytes = f.read()
                
                print(f"Extracting text from PDF using Azure Document Intelligence...")
                from document_processing.extract_text import extract_text_from_pdf_bytes
                text = extract_text_from_pdf_bytes(file_bytes)
                print(f"✓ Extracted text from PDF: {file_path}")
            except Exception as e:
                print(f"✗ PDF extraction failed: {e}")
                raise
        else:
            # Plain text file
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            print(f"✓ Loaded text from: {file_path}")
    else:
        raise ValueError("Either blob_name or file_path must be provided")
    
    print(f"Document length: {len(text)} characters")
    
    # Step 2: Chunk the text
    print("\n[2/5] Chunking text...")
    chunks = chunk_text(text, max_tokens=500)
    print(f"✓ Created {len(chunks)} chunks")
    
    # Save chunks to data/chunks directory
    chunks_dir = "data/chunks"
    os.makedirs(chunks_dir, exist_ok=True)
    for i, chunk in enumerate(chunks):
        with open(f"{chunks_dir}/chunk_{i+1}.txt", 'w', encoding='utf-8') as f:
            f.write(chunk)
    print(f"✓ Saved chunks to {chunks_dir}/")
    
    # Step 3: Generate embeddings and store locally
    print("\n[3/5] Generating embeddings and storing locally...")
    from local_vector_store import LocalVectorStore
    
    vector_store = LocalVectorStore()
    embeddings = []
    
    filename = blob_name or os.path.basename(file_path) if file_path else "unknown"
    
    for i, chunk in enumerate(chunks):
        emb = generate_embedding(chunk)
        embeddings.append(emb)
        
        # Store in local vector store (no database needed!)
        vector_store.add_chunk(
            text=chunk,
            embedding=emb,
            metadata={"filename": filename, "chunk_id": i + 1}
        )
        
        if (i + 1) % 10 == 0:
            print(f"  Processed {i + 1}/{len(chunks)} chunks")
    
    stats = vector_store.get_stats()
    print(f"✓ Generated {len(embeddings)} embeddings (768-dim)")
    print(f"✓ Stored in local vector store: {stats['total_chunks']} chunks")
    
    # Step 4: Run all agents using Azure OpenAI directly
    print("\n[4/5] Running AI agents for analysis...")
    orchestrator = AzureOpenAIOrchestrator()
    # Use full text for comprehensive analysis (or first 8000 chars if too long)
    analysis_text = text[:8000] if len(text) > 8000 else text
    results = orchestrator.run_all_agents(analysis_text)
    print(f"✓ Completed analysis with {len(results)} agents")
    
    # Step 5: Results ready (don't auto-save to file)
    print("\n[5/5] Analysis complete - results ready")
    save_to_kb(results)
    # print("✓ Results available via API (kb.md not auto-generated)")
    
    print("\n" + "=" * 60)
    print("PROCESSING COMPLETE")
    print("=" * 60)
    
    return results


def main():
    """Main execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Process RFP documents')
    parser.add_argument('--blob', help='Azure Blob name')
    parser.add_argument('--file', help='Local file path')
    
    args = parser.parse_args()
    
    if not args.blob and not args.file:
        print("Usage:")
        print("  python pipeline.py --blob <blob_name>")
        print("  python pipeline.py --file <file_path>")
        print("\nExample with local file:")
        print("  python pipeline.py --file sample_rfp.txt")
        return
    
    try:
        results = process_rfp_document(blob_name=args.blob, file_path=args.file)
        
        print("\n--- SUMMARY ---")
        for agent_name in results.keys():
            print(f"  - {agent_name}")
        
    except Exception as e:
        print(f"\n✗ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
